services:

  ai-workspace:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        - GO_VERSION=1.23.1
        - USER_NAME=${USER}
    volumes:
      - ../..:/workspaces:cached      
    command: sleep infinity

  ollama-service:
    image: ollama/ollama:0.5.1
    volumes:
      - ollama-data:/root/.ollama
    ports:
      - 11434:11434

  download-llm-1:
    image: curlimages/curl:8.6.0
    entrypoint: ["curl", "ollama-service:11434/api/pull", "-d", "{\"name\": \"qwen2.5:0.5b\"}"]
    depends_on:
      ollama-service:
        condition: service_started

  download-llm-2:
    image: curlimages/curl:8.6.0
    entrypoint: ["curl", "ollama-service:11434/api/pull", "-d", "{\"name\": \"qwen2.5:1.5b\"}"]
    depends_on:
      ollama-service:
        condition: service_started

  download-llm-3:
    image: curlimages/curl:8.6.0
    entrypoint: ["curl", "ollama-service:11434/api/pull", "-d", "{\"name\": \"qwen2.5:3b\"}"]
    depends_on:
      ollama-service:
        condition: service_started

  download-llm-5:
    image: curlimages/curl:8.6.0
    entrypoint: ["curl", "ollama-service:11434/api/pull", "-d", "{\"name\": \"snowflake-arctic-embed:33m\"}"]
    depends_on:
      ollama-service:
        condition: service_started

volumes:
  ollama-data:
